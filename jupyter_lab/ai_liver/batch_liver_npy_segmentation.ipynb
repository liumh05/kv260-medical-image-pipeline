{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è‚è„CTå›¾åƒæ‰¹é‡åˆ†å‰²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DPUåº“\n",
    "from pynq_dpu import DpuOverlay\n",
    "\n",
    "print(\"âœ“ æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬å‚æ•°\n",
    "INPUT_SIZE = (512, 512)\n",
    "IMG_MEAN = np.array([104.00698793, 116.66876762, 122.67891434], dtype=np.float32)\n",
    "MIN_AREA_THRESHOLD = 1000\n",
    "\n",
    "# æ–‡ä»¶è·¯å¾„\n",
    "MODEL_PATH = 'unet_chaos-CT_pt.xmodel'\n",
    "INPUT_PNG_FOLDERS = ['images_1', 'images_2']\n",
    "INPUT_NPY_FOLDER = 'input_npy'  # PNGè½¬NPYçš„è¾“å…¥æ–‡ä»¶å¤¹\n",
    "OUTPUT_NPY_FOLDER = 'output_npy'  # DPUæ¨ç†ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "\n",
    "print(f\"æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}\")\n",
    "print(f\"PNGè¾“å…¥æ–‡ä»¶å¤¹: {INPUT_PNG_FOLDERS}\")\n",
    "print(f\"NPYè¾“å…¥æ–‡ä»¶å¤¹: {INPUT_NPY_FOLDER}\")\n",
    "print(f\"NPYè¾“å‡ºæ–‡ä»¶å¤¹: {OUTPUT_NPY_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åˆ›å»ºç›®å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "os.makedirs(INPUT_NPY_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_NPY_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ åˆ›å»ºç›®å½•å®Œæˆ:\")\n",
    "print(f\"  - è¾“å…¥NPYç›®å½•: {INPUT_NPY_FOLDER}\")\n",
    "print(f\"  - è¾“å‡ºNPYç›®å½•: {OUTPUT_NPY_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PNG â†’ NPY è½¬æ¢å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_png_to_npy(png_path, npy_path):\n",
    "    \"\"\"å°†PNGå›¾åƒè½¬æ¢ä¸ºNPYæ ¼å¼ (å‚è€ƒliver_segmentation_dpu.ipynbçš„é¢„å¤„ç†é€»è¾‘)\"\"\"\n",
    "    # è¯»å–PNGå›¾åƒ (BGRæ ¼å¼)\n",
    "    image = cv2.imread(png_path, cv2.IMREAD_COLOR)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"æ— æ³•è¯»å–å›¾åƒ: {png_path}\")\n",
    "    \n",
    "    # è°ƒæ•´åˆ°512x512\n",
    "    image = cv2.resize(image, INPUT_SIZE, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºfloat32å¹¶è¿›è¡Œå‡å€¼å‡æ³•\n",
    "    image = np.asarray(image, np.float32)\n",
    "    image -= IMG_MEAN\n",
    "    \n",
    "    # ä¿æŒHWCæ ¼å¼ï¼Œåªæ·»åŠ æ‰¹æ¬¡ç»´åº¦ [1, 512, 512, 3]\n",
    "    image = image[np.newaxis, :, :, :]\n",
    "    \n",
    "    # ä¿å­˜ä¸ºNPYæ–‡ä»¶\n",
    "    np.save(npy_path, image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def batch_convert_png_to_npy(png_folders, npy_output_folder):\n",
    "    \"\"\"æ‰¹é‡è½¬æ¢PNGåˆ°NPY\"\"\"\n",
    "    print(\"å¼€å§‹PNG â†’ NPYè½¬æ¢...\")\n",
    "    \n",
    "    # æ‰«ææ‰€æœ‰PNGæ–‡ä»¶\n",
    "    png_files = []\n",
    "    for folder in png_folders:\n",
    "        if os.path.exists(folder):\n",
    "            files = [f for f in os.listdir(folder) \n",
    "                    if f.lower().endswith('.png')]\n",
    "            for f in files:\n",
    "                png_files.append((folder, f))\n",
    "            print(f\"{folder}: æ‰¾åˆ° {len(files)} å¼ PNGå›¾åƒ\")\n",
    "    \n",
    "    if len(png_files) == 0:\n",
    "        print(\"æ²¡æœ‰æ‰¾åˆ°PNGæ–‡ä»¶!\")\n",
    "        return []\n",
    "    \n",
    "    # æ‰¹é‡è½¬æ¢\n",
    "    converted_files = []\n",
    "    success_count = 0\n",
    "    \n",
    "    for folder, filename in tqdm(png_files, desc=\"è½¬æ¢PNGâ†’NPY\"):\n",
    "        png_path = os.path.join(folder, filename)\n",
    "        npy_filename = os.path.splitext(filename)[0] + '.npy'\n",
    "        npy_path = os.path.join(npy_output_folder, npy_filename)\n",
    "        \n",
    "        try:\n",
    "            convert_png_to_npy(png_path, npy_path)\n",
    "            converted_files.append((npy_filename, npy_path))\n",
    "            success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\" è½¬æ¢å¤±è´¥ {filename}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n è½¬æ¢å®Œæˆ: {success_count}/{len(png_files)} ä¸ªæ–‡ä»¶\")\n",
    "    return converted_files\n",
    "\n",
    "\n",
    "print(\"âœ“ è½¬æ¢å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_image(npy_path):\n",
    "    \"\"\"åŠ è½½NPYæ ¼å¼çš„é¢„å¤„ç†å›¾åƒ\"\"\"\n",
    "    image = np.load(npy_path)\n",
    "    return image\n",
    "\n",
    "\n",
    "def postprocess_output(output, original_shape=None):\n",
    "    \"\"\"åå¤„ç†æ¨¡å‹è¾“å‡ºï¼Œè¿”å›NPYæ ¼å¼çš„åˆ†å‰²ç»“æœ\"\"\"\n",
    "    # Argmaxå¾—åˆ°åˆ†å‰²ç»“æœ\n",
    "    mask = np.argmax(output[0], axis=-1)  # [512, 512]\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    \n",
    "    # è¿é€šåŸŸåˆ†æï¼Œä¿ç•™æœ€å¤§åŒºåŸŸ\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask)\n",
    "    if num_labels > 1:\n",
    "        areas = stats[1:, cv2.CC_STAT_AREA]\n",
    "        max_idx = np.argmax(areas) + 1\n",
    "        if areas[max_idx-1] > MIN_AREA_THRESHOLD:\n",
    "            mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "            mask[labels == max_idx] = 255\n",
    "        else:\n",
    "            mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸\n",
    "    if original_shape is not None:\n",
    "        mask = cv2.resize(mask, (original_shape[1], original_shape[0]), \n",
    "                         interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "print(\"âœ“ NPYå¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ‰§è¡ŒPNG â†’ NPYè½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œæ‰¹é‡è½¬æ¢\n",
    "npy_files = batch_convert_png_to_npy(INPUT_PNG_FOLDERS, INPUT_NPY_FOLDER)\n",
    "\n",
    "if npy_files:\n",
    "    print(f\"\\n è½¬æ¢åçš„NPYæ–‡ä»¶ä¿å­˜åœ¨: {INPUT_NPY_FOLDER}\")\n",
    "    print(f\" å‰5ä¸ªæ–‡ä»¶:\")\n",
    "    for i, (filename, _) in enumerate(npy_files[:5]):\n",
    "        print(f\"  {i+1}. {filename}\")\n",
    "    \n",
    "    # æ£€æŸ¥ç¬¬ä¸€ä¸ªNPYæ–‡ä»¶çš„å½¢çŠ¶\n",
    "    first_npy = np.load(npy_files[0][1])\n",
    "    print(f\"\\n NPYæ–‡ä»¶å½¢çŠ¶: {first_npy.shape}\")\n",
    "    print(f\" æ•°æ®ç±»å‹: {first_npy.dtype}\")\n",
    "    print(f\" å€¼èŒƒå›´: [{first_npy.min():.3f}, {first_npy.max():.3f}]\")\n",
    "else:\n",
    "    print(\" æ²¡æœ‰æˆåŠŸè½¬æ¢çš„NPYæ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åŠ è½½DPUæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if npy_files:\n",
    "    print(\"æ­£åœ¨åŠ è½½DPU...\")\n",
    "    \n",
    "    # åŠ è½½DPU overlay\n",
    "    overlay = DpuOverlay(\"dpu.bit\")\n",
    "    print(\"âœ“ DPU overlayåŠ è½½æˆåŠŸ\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    overlay.load_model(MODEL_PATH)\n",
    "    print(\"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "    \n",
    "    # è·å–DPU runner\n",
    "    dpu = overlay.runner\n",
    "    input_tensors = dpu.get_input_tensors()\n",
    "    output_tensors = dpu.get_output_tensors()\n",
    "    \n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {input_tensors[0].dims}\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {output_tensors[0].dims}\")\n",
    "else:\n",
    "    print(\" æ²¡æœ‰NPYæ–‡ä»¶ï¼Œè·³è¿‡DPUåŠ è½½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ‰¹é‡DPUæ¨ç† (NPYæ ¼å¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if npy_files and 'dpu' in locals():\n",
    "    print(\"å¼€å§‹æ‰¹é‡DPUæ¨ç†...\")\n",
    "    \n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    stats = {\n",
    "        'total': len(npy_files),\n",
    "        'success': 0,\n",
    "        'failed': 0,\n",
    "        'times': [],\n",
    "        'liver_pixels': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æ‰¹é‡å¤„ç†å¾ªç¯\n",
    "    for npy_filename, npy_path in tqdm(npy_files, desc=\"DPUæ¨ç†\"):\n",
    "        try:\n",
    "            inference_start = time.time()\n",
    "            \n",
    "            # 1. åŠ è½½NPYæ–‡ä»¶\n",
    "            input_array = load_npy_image(npy_path)\n",
    "            \n",
    "            # 2. DPUæ¨ç†\n",
    "            input_data = [input_array.astype(np.float32)]\n",
    "            output_data = [np.empty(tuple(output_tensors[0].dims), dtype=np.float32)]\n",
    "            \n",
    "            job_id = dpu.execute_async(input_data, output_data)\n",
    "            dpu.wait(job_id)\n",
    "            output = output_data[0]\n",
    "            \n",
    "            # 3. åå¤„ç†\n",
    "            mask = postprocess_output(output)\n",
    "            \n",
    "            # 4. ä¿å­˜NPYæ ¼å¼ç»“æœ\n",
    "            base_name = os.path.splitext(npy_filename)[0]\n",
    "            output_npy_path = os.path.join(OUTPUT_NPY_FOLDER, base_name + '_mask.npy')\n",
    "            np.save(output_npy_path, mask)\n",
    "            \n",
    "            # ç»Ÿè®¡ä¿¡æ¯\n",
    "            inference_time = time.time() - inference_start\n",
    "            liver_pixels = np.sum(mask > 0)\n",
    "            \n",
    "            stats['success'] += 1\n",
    "            stats['times'].append(inference_time)\n",
    "            stats['liver_pixels'].append(liver_pixels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            stats['failed'] += 1\n",
    "            print(f\" æ¨ç†å¤±è´¥ {npy_filename}: {str(e)}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"æ‰¹é‡DPUæ¨ç†å®Œæˆ!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ\n",
    "    print(f\"\\n æ¨ç†ç»Ÿè®¡:\")\n",
    "    print(f\"  æ€»æ–‡ä»¶æ•°: {stats['total']}\")\n",
    "    print(f\"  æˆåŠŸæ¨ç†: {stats['success']}\")\n",
    "    print(f\"  æ¨ç†å¤±è´¥: {stats['failed']}\")\n",
    "    print(f\"  æˆåŠŸç‡: {stats['success']/stats['total']*100:.1f}%\")\n",
    "    \n",
    "    if stats['times']:\n",
    "        times = np.array(stats['times'])\n",
    "        print(f\"\\n æ€§èƒ½ç»Ÿè®¡:\")\n",
    "        print(f\"  æ€»æ¨ç†æ—¶é—´: {total_time:.2f} ç§’\")\n",
    "        print(f\"  å¹³å‡æ¨ç†é€Ÿåº¦: {stats['success']/total_time:.2f} å›¾åƒ/ç§’\")\n",
    "        print(f\"  å•å¼ å¹³å‡æ—¶é—´: {np.mean(times):.3f} Â± {np.std(times):.3f} ç§’\")\n",
    "        print(f\"  æœ€å¿«æ¨ç†æ—¶é—´: {np.min(times):.3f} ç§’\")\n",
    "        print(f\"  æœ€æ…¢æ¨ç†æ—¶é—´: {np.max(times):.3f} ç§’\")\n",
    "    \n",
    "    if stats['liver_pixels']:\n",
    "        liver_count = sum(1 for p in stats['liver_pixels'] if p > 0)\n",
    "        nonzero_pixels = [p for p in stats['liver_pixels'] if p > 0]\n",
    "        \n",
    "        print(f\"\\n è‚è„æ£€æµ‹ç»Ÿè®¡:\")\n",
    "        print(f\"  æ£€æµ‹åˆ°è‚è„: {liver_count}/{stats['success']} ({liver_count/stats['success']*100:.1f}%)\")\n",
    "        \n",
    "        if nonzero_pixels:\n",
    "            print(f\"  å¹³å‡è‚è„åƒç´ : {np.mean(nonzero_pixels):.0f}\")\n",
    "            print(f\"  è‚è„åƒç´ èŒƒå›´: {np.min(nonzero_pixels)} - {np.max(nonzero_pixels)}\")\n",
    "    \n",
    "    print(f\"\\n è¾“å…¥NPYæ–‡ä»¶: {INPUT_NPY_FOLDER}\")\n",
    "    print(f\" è¾“å‡ºNPYæ–‡ä»¶: {OUTPUT_NPY_FOLDER}\")\n",
    "    \n",
    "else:\n",
    "    print(\" æ²¡æœ‰NPYæ–‡ä»¶æˆ–DPUæœªåŠ è½½ï¼Œè·³è¿‡æ¨ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æ˜¾ç¤ºå¤„ç†ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºä¸€äº›å¤„ç†ç»“æœ\n",
    "if os.path.exists(OUTPUT_NPY_FOLDER):\n",
    "    output_files = [f for f in os.listdir(OUTPUT_NPY_FOLDER) if f.endswith('_mask.npy')]\n",
    "    \n",
    "    if output_files:\n",
    "        print(f\"æ‰¾åˆ° {len(output_files)} ä¸ªè¾“å‡ºæ–‡ä»¶\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰3ä¸ªç»“æœ (æ”¹ä¸º3åˆ—ï¼šåŸå›¾ã€æ©ç ã€å åŠ æ•ˆæœ)\n",
    "        fig, axes = plt.subplots(len(output_files[:3]), 3, figsize=(15, 5*len(output_files[:3])))\n",
    "        if len(output_files[:3]) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, output_file in enumerate(output_files[:3]):\n",
    "            # åŠ è½½å¯¹åº”çš„åŸå§‹NPYå’Œè¾“å‡ºNPY\n",
    "            input_name = output_file.replace('_mask.npy', '.npy')\n",
    "            input_path = os.path.join(INPUT_NPY_FOLDER, input_name)\n",
    "            output_path = os.path.join(OUTPUT_NPY_FOLDER, output_file)\n",
    "            \n",
    "            if os.path.exists(input_path) and os.path.exists(output_path):\n",
    "                # åŠ è½½æ•°æ®\n",
    "                input_data = np.load(input_path)[0]  # [512, 512, 3]\n",
    "                mask = np.load(output_path)  # [512, 512]\n",
    "                \n",
    "                # åå‘é¢„å¤„ç†ç”¨äºæ˜¾ç¤º\n",
    "                display_image = (input_data + IMG_MEAN).astype(np.uint8)\n",
    "                \n",
    "                # æ˜¾ç¤ºåŸå§‹å›¾åƒ\n",
    "                axes[i, 0].imshow(cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB))\n",
    "                axes[i, 0].set_title(f'original picture: {os.path.splitext(input_name)[0]}')\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                # æ˜¾ç¤ºåˆ†å‰²ç»“æœ\n",
    "                axes[i, 1].imshow(mask, cmap='gray')\n",
    "                axes[i, 1].set_title(f'Segmentation mask')\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                # æ·»åŠ å åŠ æ•ˆæœæ˜¾ç¤º \n",
    "                if np.sum(mask > 0) > 0:  # å¦‚æœæœ‰è‚è„åŒºåŸŸ\n",
    "                    overlay_image = display_image.copy()\n",
    "                    # åˆ›å»ºå½©è‰²æ©ç ï¼ˆç»¿è‰²ï¼‰\n",
    "                    colored_mask = np.zeros_like(overlay_image)\n",
    "                    colored_mask[mask > 0] = [0, 255, 0]  # ç»¿è‰²é€šé“\n",
    "                    # å åŠ æ˜¾ç¤º\n",
    "                    overlay_result = cv2.addWeighted(overlay_image, 0.7, colored_mask, 0.3, 0)\n",
    "                    axes[i, 2].imshow(cv2.cvtColor(overlay_result, cv2.COLOR_BGR2RGB))\n",
    "                    axes[i, 2].set_title('Layering Effect (green)')\n",
    "                else:  # å¦‚æœæ²¡æœ‰è‚è„åŒºåŸŸ\n",
    "                    axes[i, 2].text(0.5, 0.5, 'not detect', \n",
    "                                     ha='center', va='center', transform=axes[i, 2].transAxes, fontsize=12)\n",
    "                    axes[i, 2].set_title('No liver area')\n",
    "                \n",
    "                axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ä¿¡æ¯\n",
    "        first_output = np.load(os.path.join(OUTPUT_NPY_FOLDER, output_files[0]))\n",
    "        print(f\"\\n è¾“å‡ºNPYæ–‡ä»¶å½¢çŠ¶: {first_output.shape}\")\n",
    "        print(f\" è¾“å‡ºæ•°æ®ç±»å‹: {first_output.dtype}\")\n",
    "        print(f\" è¾“å‡ºå€¼èŒƒå›´: [{first_output.min()}, {first_output.max()}]\")\n",
    "        \n",
    "        # ç»Ÿè®¡æœ‰è‚è„åŒºåŸŸçš„å›¾åƒæ•°é‡ \n",
    "        liver_count = 0\n",
    "        for output_file in output_files:\n",
    "            mask = np.load(os.path.join(OUTPUT_NPY_FOLDER, output_file))\n",
    "            if np.sum(mask > 0) > 0:\n",
    "                liver_count += 1\n",
    "        \n",
    "        print(f\"\\n å åŠ æ•ˆæœç»Ÿè®¡:\")\n",
    "        print(f\"  æœ‰è‚è„åŒºåŸŸçš„å›¾åƒ: {liver_count}/{len(output_files)} ({liver_count/len(output_files)*100:.1f}%)\")\n",
    "        \n",
    "        if liver_count > 0:\n",
    "            # è®¡ç®—è‚è„åƒç´ ç»Ÿè®¡\n",
    "            total_pixels = 0\n",
    "            for output_file in output_files:\n",
    "                mask = np.load(os.path.join(OUTPUT_NPY_FOLDER, output_file))\n",
    "                total_pixels += np.sum(mask > 0)\n",
    "            \n",
    "            print(f\"  æ€»è‚è„åƒç´ æ•°: {total_pixels:,}\")\n",
    "            print(f\"  å¹³å‡è‚è„åƒç´ /å›¾åƒ: {total_pixels/liver_count:.0f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"æ²¡æœ‰æ‰¾åˆ°è¾“å‡ºæ–‡ä»¶\")\n",
    "else:\n",
    "    print(\"è¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ¸…ç†èµ„æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡Šæ”¾DPUèµ„æº\n",
    "try:\n",
    "    del dpu\n",
    "    del overlay\n",
    "    print(\"âœ“ DPUèµ„æºå·²é‡Šæ”¾\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n NPYæ ¼å¼æ‰¹é‡å¤„ç†å®Œæˆ!\")\n",
    "print(f\" è¾“å…¥NPYæ–‡ä»¶: {INPUT_NPY_FOLDER}\")\n",
    "print(f\" è¾“å‡ºNPYæ–‡ä»¶: {OUTPUT_NPY_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½»åº•æ¸…ç†å†…å­˜ - å‡å°‘å†…å­˜å ç”¨\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"æ¸…ç†å†…å­˜ä¸­çš„å˜é‡å’Œç¼“å­˜\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§¹ å¼€å§‹æ¸…ç†å†…å­˜...\")\n",
    "    \n",
    "    # 1. æ¸…ç†DPUç›¸å…³èµ„æº (å¦‚æœè¿˜æœ‰æ®‹ç•™)\n",
    "    dpu_vars = ['dpu', 'overlay', 'input_tensors', 'output_tensors']\n",
    "    for var in dpu_vars:\n",
    "        if var in globals():\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"âœ“ åˆ é™¤å˜é‡: {var}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 2. æ¸…ç†å¤§å‹æ•°ç»„å’Œåˆ—è¡¨\n",
    "    array_vars = ['input_array', 'output_data', 'mask', 'first_npy', 'output']\n",
    "    for var in array_vars:\n",
    "        if var in globals():\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"âœ“ åˆ é™¤æ•°ç»„: {var}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 3. æ¸…ç†æ–‡ä»¶åˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯\n",
    "    list_vars = ['npy_files', 'output_files', 'converted_files']\n",
    "    for var in list_vars:\n",
    "        if var in globals():\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"âœ“ åˆ é™¤åˆ—è¡¨: {var}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 4. æ¸…ç†ç»Ÿè®¡å­—å…¸\n",
    "    dict_vars = ['stats']\n",
    "    for var in dict_vars:\n",
    "        if var in globals():\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"âœ“ åˆ é™¤å­—å…¸: {var}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 5. å¼ºåˆ¶åƒåœ¾å›æ”¶\n",
    "    collected = gc.collect()\n",
    "    print(f\"âœ“ åƒåœ¾å›æ”¶å®Œæˆï¼Œæ¸…ç†äº† {collected} ä¸ªå¯¹è±¡\")\n",
    "    \n",
    "    # 6. æ¸…ç†matplotlibç¼“å­˜\n",
    "    try:\n",
    "        plt.close('all')\n",
    "        print(\"âœ“ æ¸…ç†matplotlibå›¾å½¢ç¼“å­˜\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 7. æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        memory_mb = memory_info.rss / 1024 / 1024\n",
    "        \n",
    "        print(f\"\\nğŸ“Š å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ:\")\n",
    "        print(f\"  RSSå†…å­˜: {memory_mb:.1f} MB\")\n",
    "        print(f\"  VMSå†…å­˜: {memory_info.vms / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        if memory_mb > 1000:  # å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡1GB\n",
    "            print(\"âš ï¸  å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®é‡å¯kernel\")\n",
    "        else:\n",
    "            print(\"âœ“ å†…å­˜ä½¿ç”¨æ­£å¸¸\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"  (psutilæœªå®‰è£…ï¼Œæ— æ³•æ˜¾ç¤ºå†…å­˜ä¿¡æ¯)\")\n",
    "    \n",
    "    return memory_mb if 'memory_mb' in locals() else None\n",
    "\n",
    "# æ‰§è¡Œå†…å­˜æ¸…ç†\n",
    "final_memory = cleanup_memory()\n",
    "\n",
    "print(f\"\\nğŸ‰ å†…å­˜æ¸…ç†å®Œæˆ!\")\n",
    "print(f\"ğŸ“ è¾“å…¥NPYæ–‡ä»¶ä¿ç•™åœ¨: {INPUT_NPY_FOLDER}\")\n",
    "print(f\"ğŸ“ è¾“å‡ºNPYæ–‡ä»¶ä¿ç•™åœ¨: {OUTPUT_NPY_FOLDER}\")\n",
    "print(f\"\\nğŸ’¡ æç¤º: å¦‚éœ€è¿›ä¸€æ­¥é‡Šæ”¾å†…å­˜ï¼Œå¯ä»¥é‡å¯Jupyter kernel\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}